{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest! :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from itertools import compress\n",
    "\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "train_split = .8\n",
    "show_plot = True\n",
    "num_pred_jokes = 10 # number of jokes you want to predict for a user\n",
    "\n",
    "# using a random person for demo purposes. can be changed\n",
    "sample_user = {'major':'Computer Science', 'age':23, 'birth_country':\"India\", 'gender':\"Male\", \\\n",
    "               'id':56, 'genre1':\"Sports\", 'genre2':\"Math\", 'type':'Pick-up Line', 'music':\"Rap\", 'movie':\"Action\"}\n",
    "c = 15 # for sample weights\n",
    "train = False # either train/test split, or use all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lsa_fn(X_tfidf, dim_reduce = 20, print_var=False):\n",
    "    from sklearn.decomposition import TruncatedSVD \n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    \n",
    "    dim_reduce: the number of columns you expect for the results\n",
    "    X_tfidf: ti-idf matrix \n",
    "    \n",
    "    OUTPUT:\n",
    "    matrix with reduced dim (should be number_of_jokes x dim_reduce) \n",
    "    \"\"\"\n",
    "    \n",
    "    lsa = TruncatedSVD(dim_reduce, algorithm = 'arpack')\n",
    "\n",
    "    # X_tfidf : 153 x 788 tf-idf matrix\n",
    "    dtm_lsa = lsa.fit_transform(X_tfidf)\n",
    "\n",
    "    #reduced matrix (combine this matrix w/ other features)\n",
    "    dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "    \n",
    "    if print_var:\n",
    "        print(str(lsa.explained_variance_.cumsum()[-1] * 100) + \"%\")\n",
    "    \n",
    "    return(dtm_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_feature_df(tfidf, add_df):\n",
    "    tfidf_df = pd.DataFrame(tfidf)\n",
    "    tfidf_columns = [\"tfidf\" + str(i) for i in range((tfidf_df.shape[1]))]\n",
    "    tfidf_df.columns = tfidf_columns\n",
    "    feat = pd.concat([add_df, tfidf_df], axis=1)\n",
    "    feat.rename(columns = {'id':'joke_id'}, inplace = True)\n",
    "    \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_low_variance_users(df):\n",
    "    user_var = {}\n",
    "    for rater in df.joke_rater_id.unique():\n",
    "        entries = df[(df['joke_rater_id']==rater)]\n",
    "        ratings = entries.rating\n",
    "        var = np.nanvar(ratings)\n",
    "        if np.isnan(var) == False:\n",
    "            user_var[rater] = var\n",
    "            #print(str(rater) + \": \" + str(np.nanvar(ratings)))\n",
    "\n",
    "    bad_keys = dict((k, v) for k, v in user_var.items() if v < 0.4).keys()\n",
    "\n",
    "    df = df[~df['joke_rater_id'].isin(bad_keys)].reset_index(drop=True) # remove low variance users\n",
    "    # df = df.loc[0:13398] # remove single entry NaN users -- will cause an issue if the database changes\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def impute_NA(df):\n",
    "    which_drop = df[df.isnull().sum(axis=1) > 2].index\n",
    "    new_df = df.drop(which_drop)\n",
    "\n",
    "    modes = new_df.mode()\n",
    "    new_df.birth_country = new_df.birth_country.fillna(\"United States\")\n",
    "    new_df.preferred_joke_type = new_df.preferred_joke_type.fillna(\"Puns\")\n",
    "    new_df.preferred_joke_genre2 = new_df.preferred_joke_genre.fillna(\"Programming\")\n",
    "    new_df = new_df.drop(new_df[new_df.joke_type.isnull() == True].index)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_category_to_dummy(df):\n",
    "    \n",
    "    # ignoes all numeric entries\n",
    "    ignore_col = [i for i in range((df.shape[1])) if (df.iloc[:,i].dtype == np.int64) or (df.iloc[:,i].dtype == np.float64)]\n",
    "    ignore_col.extend([list(df.columns).index(\"joke_id\")])\n",
    "    ignore_col = sorted(ignore_col)\n",
    "    \n",
    "    #new_df.iloc[:,string_col] = pd.get_dummies(new_df.iloc[:,string_col])\n",
    "    string_col = []\n",
    "    for i in range((df.shape[1])):\n",
    "        if i not in ignore_col:\n",
    "            string_col.append(i)\n",
    "    # same thing as\n",
    "    #string_col = [3, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "    \n",
    "    df = pd.concat([df.iloc[:,ignore_col], pd.get_dummies(df.iloc[:,string_col])], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lasso_selection(df2):\n",
    "    \n",
    "    disclude_col = ['rating', 'joke_rater_id', 'joke_id']\n",
    "    features = [col for col in df2.columns if col not in disclude_col]\n",
    "\n",
    "    lasso = Lasso(alpha=.001, random_state=2).fit(df2[features], df2.rating)\n",
    "    model = SelectFromModel(lasso, prefit=True)\n",
    "\n",
    "    lasso_X = model.transform(df2[features])\n",
    "\n",
    "    for i, feature in zip(model.get_support(), features): # get headers, since they get lost after lasso\n",
    "        if i:\n",
    "            disclude_col.append(feature)\n",
    "\n",
    "    df3 = pd.concat([df2.rating.reset_index(drop=True), df2.joke_rater_id.reset_index(drop=True), \n",
    "                    df2.joke_id.reset_index(drop=True), pd.DataFrame(lasso_X)], axis=1)\n",
    "    df3.columns = disclude_col\n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weigh_samples_vector(df, user_id=None, c=2):\n",
    "    '''\n",
    "    Sets all weights equal to 1.\n",
    "    If user_id exists in database/csv, then increase weights to c, where c >= 1.\n",
    "    Works if user already exists (already rated jokes) or if new user.\n",
    "    Return np array that is to be used in rf.fit\n",
    "    c is tuneable to how much you want to weight the user's personal ratings.\n",
    "    '''\n",
    "    vector_length = df.shape[0]\n",
    "    vector = np.ones(vector_length)\n",
    "    if user_id in df.joke_rater_id.unique(): # is user already exists in database, increase weights\n",
    "        idx = df3[df3.joke_rater_id == user_id].index\n",
    "        vector[idx] = c # increase -- set to c\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pred_vs_actual(y_pred, y_test):\n",
    "    ax = sns.regplot(x=y_test, y=y_pred.astype('float'), scatter_kws={'alpha':0.1})\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Predicted Rating vs. Actual Rating\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorize_multiclass(label, user_label, entry, features, numRow):\n",
    "    '''\n",
    "    label is what the dummy string category name begins with, i.e. \"birth_country_\"\n",
    "    user_label is the quantity inside the user_dict, accessed by specific key, i.e. user_dict['birth_country']\n",
    "    '''\n",
    "    user_class = label + str(user_label) \n",
    "    avail_labels = list(compress(features, [item.startswith(label) for item in features]))\n",
    "    label_cols = [i for i, x in enumerate(entry.columns) if x in avail_labels]\n",
    "\n",
    "    for col in label_cols:\n",
    "        entry.iloc[:, col] = np.repeat(0, numRow) # set all to 0 for blank slate\n",
    "    if user_class in avail_labels:\n",
    "        entry.iloc[:,user_class] = np.repeat(1, numRow) # if user class present, set to 1\n",
    "        \n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert data into one-hot\n",
    "\n",
    "# assume data is a dict user_dict\n",
    "def convert_sample_onehot(user_dict, df, features):\n",
    "    '''\n",
    "    Works with new user or existing user.\n",
    "    Input: rater_id = user_dict (age, gender, birth_country, major, id)\n",
    "           df = combined, cleaned dataframe (df3)\n",
    "           \n",
    "    Converts user data into variables inside the dataframe (i.e. df3) so you can pass into the random forest.\n",
    "    '''\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    if user_dict['id'] in df.joke_rater_id.unique(): # is user already exists in database\n",
    "        return df[df.joke_rater_id == user_dict['id']]        \n",
    "    \n",
    "    entry = df[df.joke_rater_id == 476] # chose 476 randomly because they rated all 153 jokes\n",
    "    numRow = entry.shape[0]\n",
    "    \n",
    "    if user_dict['gender'] == \"Male\": # gender\n",
    "        entry.gender_Female = np.repeat(a=0, repeats=numRow)\n",
    "    \n",
    "    entry.age = np.repeat(user_dict['age'], numRow) # age\n",
    "    entry.joke_rater_id = np.repeat(user_dict['id'], numRow)\n",
    "    \n",
    "    ## COUNTRY\n",
    "    entry = categorize_multiclass(\"birth_country_\", \"birth_country\", entry, features, numRow)\n",
    "    \n",
    "    ## MAJOR\n",
    "    entry = categorize_multiclass(\"major_\", \"major\", entry, features, numRow)\n",
    "    \n",
    "    ## PREFERRED JOKE GENRE 1\n",
    "    entry = categorize_multiclass(\"preferred_joke_genre_\", \"genre1\", entry, features, numRow)\n",
    "    \n",
    "    ## PREFERRED JOKE GENRE 2\n",
    "    entry = categorize_multiclass(\"preferred_joke_genre2_\", \"genre2\", entry, features, numRow)\n",
    "    \n",
    "    ## PREFERRED JOKE TYPE\n",
    "    entry = categorize_multiclass(\"preferred_joke_type_\", \"type\", entry, features, numRow)\n",
    "    \n",
    "    ## MOVIE\n",
    "    entry = categorize_multiclass(\"favorite_movie_genre_\", \"movie\", entry, features, numRow)\n",
    "    \n",
    "    ## MUSIC\n",
    "    entry = categorize_multiclass(\"favorite_music_genre_\", \"music\", entry, features, numRow)\n",
    "        \n",
    "    return entry\n",
    "\n",
    "#entry = df3[df3.joke_rater_id == 476]\n",
    "#numRow = entry.shape[0]\n",
    "#categorize_multiclass(\"preferred_joke_genre_\", \"joke1\", entry, features, numRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topk_jokes(user_df, rf, joke_ids, features, k=10):\n",
    "    '''\n",
    "    Returns top k jokes for user (default=10).\n",
    "    user_df is output from convert_sample_onehot.\n",
    "    Assumes random forest rf is already trained.\n",
    "    '''\n",
    "    preds = rf.predict(user_df[features])\n",
    "    \n",
    "    df = pd.DataFrame(joke_ids)\n",
    "    df['pred'] = preds\n",
    "        \n",
    "    return (df.sort_values(by='pred', ascending=False).head(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(df3, user_id):\n",
    "\n",
    "    unique_rater = df3.joke_rater_id.unique() # all unique users\n",
    "    train_size = round(len(unique_rater) * train_split) # 80/20 train/test split!\n",
    "\n",
    "    train_idx = np.random.choice(unique_rater, train_size, replace=False) # get randomly train_size number of users to put into train\n",
    "    test_idx = [i for i in unique_rater if i not in train_idx] # remaining users go to test\n",
    "\n",
    "    train_df = df3.loc[df3['joke_rater_id'].isin(train_idx)]\n",
    "    test_df = df3.loc[df3['joke_rater_id'].isin(test_idx)]\n",
    "    \n",
    "    # time to run random forest regressor\n",
    "    rf = train_rf(train_df, user_id)\n",
    "    \n",
    "    # testing\n",
    "    y_test = test_df.rating\n",
    "    \n",
    "    disclude = ['joke_rater_id', 'rating', 'joke_id']\n",
    "    features = [col for col in df3.columns if col not in disclude]\n",
    "    \n",
    "    y_pred = rf.predict(test_df[features]).astype('float')\n",
    "    print(\"Test MSE is: \" + str(mse(y_test, y_pred)))\n",
    "    \n",
    "    #df = pd.DataFrame.from_dict({'y_test':y_test, 'y_pred': y_pred})\n",
    "    #df.to_csv('actual_vs_pred.csv')\n",
    "    \n",
    "     # see distribution of predicted joke score vs. actual joke value\n",
    "    if show_plot:\n",
    "        plot_pred_vs_actual(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_rf(df3, user_id, print_importance=False):\n",
    "    y = df3.rating\n",
    "    Y_list = list(y.values)\n",
    "    \n",
    "    disclude = ['joke_rater_id', 'rating', 'joke_id']\n",
    "    features = [col for col in df3.columns if col not in disclude]\n",
    "\n",
    "    sample_weights = weigh_samples_vector(df=df3, user_id=user_id, c=c) # weigh user's ratings more\n",
    "    sample_weights = np.ravel(normalize(sample_weights.reshape((-1, 1)), axis=0))\n",
    "    min_weight = min(sample_weights) + 0.001\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=50, max_features='sqrt', random_state=42, \\\n",
    "                                   max_depth=10, min_weight_fraction_leaf=min_weight) # tuneable parameters\n",
    "    rf.fit(df3[features], y, sample_weight=sample_weights)\n",
    "    \n",
    "    if print_importance:\n",
    "        # see what factors are most important\n",
    "        s = pd.DataFrame((rf.feature_importances_))\n",
    "\n",
    "        s = s.transpose()\n",
    "        s.columns = features\n",
    "        s = s.transpose()\n",
    "\n",
    "        print(\"10 most important features: \")\n",
    "        print(s.sort_values(by=0, ascending=False).head(10))\n",
    "        (s.sort_values(by=0, ascending=False)).to_csv('feat_import.csv')\n",
    "    \n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preds(user, df3, rf, k):\n",
    "    \n",
    "    ## QUERYING JOKE PREDICTIONS FOR NEW USER\n",
    "    \n",
    "    disclude = ['joke_rater_id', 'rating', 'joke_id']\n",
    "    features = [col for col in df3.columns if col not in disclude]\n",
    "    \n",
    "    user_df = convert_sample_onehot(user, df3, features)\n",
    "    joke_ids = df3[df3.joke_rater_id == 476].joke_id \n",
    "    preds = get_topk_jokes(user_df, rf, features=features, joke_ids=joke_ids, k=k)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_df():\n",
    "    # 3 dfs from csvs\n",
    "    rater_df = pd.read_csv(\"JokeRater.csv\")\n",
    "    rating_df = pd.read_csv(\"JokeRating.csv\")\n",
    "    joke_df = pd.read_csv(\"Joke.csv\")\n",
    "    \n",
    "    ## PRE-PROCESSING DATAAAAAA\n",
    "    \n",
    "    # change column names for merging purposes\n",
    "    rater_df.rename(columns = {'id':'joke_rater_id'}, inplace = True)\n",
    "    joke_df.rename(columns = {'id':'joke_id'}, inplace = True)\n",
    "    joke_df['joke_id'] = joke_df['joke_id'].astype(float)\n",
    "    rater_df = rater_df.drop('joke_submitter_id', axis=1)\n",
    "    joke_df = joke_df.drop('joke_submitter_id', axis=1)\n",
    "    joke_df = joke_df.drop('joke_source', axis=1)\n",
    "    \n",
    "    # add tf-idf features\n",
    "    feature_df = pd.read_csv(\"feature_tfidf.csv\")\n",
    "    add_features = feature_df.iloc[:,1:5] # misc. features like avg length, num words\n",
    "\n",
    "    X_tfidf = feature_df.iloc[:,6:]\n",
    "    \n",
    "    # use LSA to reduce down number of tfidf columns\n",
    "    reduced_tfidf = lsa_fn(X_tfidf, 100) # 79.3% variance explained\n",
    "    \n",
    "    feat = preprocess_feature_df(add_df=add_features, tfidf=reduced_tfidf)\n",
    "\n",
    "    joke_df = pd.merge(joke_df, feat, on='joke_id', how='outer') # combine new features to joke dataframe\n",
    "    \n",
    "    # combine joke raters with their ratings\n",
    "    df = pd.merge(rating_df, rater_df, on=\"joke_rater_id\", how=\"outer\")\n",
    "    df = df.drop('id', axis=1)\n",
    "    \n",
    "    # deal with low variance users\n",
    "    df = remove_low_variance_users(df)\n",
    "    \n",
    "    # finally, add jokes in\n",
    "    df = pd.merge(df, joke_df, on='joke_id', how='outer')\n",
    "    df = df.drop('subject', axis=1)\n",
    "    df = df.drop('joke_text', axis=1)\n",
    "    \n",
    "    # get rid of high NaN entries, and replace categories with modes\n",
    "    df = impute_NA(df)\n",
    "\n",
    "    # convert categorical variables into dummies (one-hot)\n",
    "    df2 = change_category_to_dummy(df)\n",
    "    \n",
    "    # lasso feature selection\n",
    "    df3 = lasso_selection(df2)\n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTO USE:\\nuser_df = convert_sample_onehot(sample_user, df3, features)\\nten_random_jokes = [505, 506, 507, 508, 509, 511, 512, 513, 514, 515] # later changed to actual 10 joke_ids returned\\nratings = [3, 5, 3, 1, 2, None, 4, None, None, 3] # replace with actual values later\\n\\nappend_new_rows(df3, sample_user, ratings, ten_random_jokes)\\n'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def append_new_rows(df3, user_dict, returned_ratings, returned_joke_ids):\n",
    "    '''\n",
    "    user_dict: has same structure as sample_user\n",
    "    df3 is the finished dataframe after prepare_df()\n",
    "    returned_ratings: list of ratings returned back.\n",
    "    returned_joke_ids: list of joke IDs that the user rated\n",
    "    \n",
    "    ASSUME ratings are in the same order as the returned_joke_ids\n",
    "    \n",
    "    Function: Appends user's responses to the master dataframe (df3).\n",
    "    '''\n",
    "    disclude = ['joke_rater_id', 'rating', 'joke_id']\n",
    "    features = [col for col in df3.columns if col not in disclude]\n",
    "\n",
    "    user_df = convert_sample_onehot(user_dict, df3, features) # creates template for user\n",
    "    relevant_jokes = user_df[user_df.joke_id.isin(returned_joke_ids)]\n",
    "    relevant_jokes.rating = returned_ratings # update ratings with the correct ratings user inputted\n",
    "    relevant_jokes = relevant_jokes.drop(relevant_jokes[relevant_jokes.rating.isnull() == True].index) # remove NaN ratings\n",
    "    \n",
    "    df3 = df3.append(relevant_jokes) # append new jokes to the dataframe\n",
    "    \n",
    "    return df3\n",
    "\n",
    "\n",
    "'''\n",
    "TO USE:\n",
    "user_df = convert_sample_onehot(sample_user, df3, features)\n",
    "ten_random_jokes = [505, 506, 507, 508, 509, 511, 512, 513, 514, 515] # later changed to actual 10 joke_ids returned\n",
    "ratings = [3, 5, 3, 1, 2, None, 4, None, None, 3] # replace with actual values later\n",
    "\n",
    "append_new_rows(df3, sample_user, ratings, ten_random_jokes)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cindy\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: RuntimeWarning: Degrees of freedom <= 0 for slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most important features: \n",
      "                                       0\n",
      "age                             0.110243\n",
      "gender_Female                   0.054919\n",
      "preferred_joke_genre2_Math      0.046944\n",
      "preferred_joke_type_Fun fact    0.045021\n",
      "preferred_joke_genre_Math       0.043559\n",
      "favorite_music_genre_Classical  0.025672\n",
      "preferred_joke_type_Punch line  0.024969\n",
      "favorite_music_genre_Rap        0.023657\n",
      "favorite_movie_genre_Mystery    0.020454\n",
      "major_Physics                   0.020052\n"
     ]
    }
   ],
   "source": [
    "df3 = prepare_df()\n",
    "\n",
    "def query(user):\n",
    "    \n",
    "    ## RANDOM FORESTTTT\n",
    "    \n",
    "    if train: # train/test to get test MSE\n",
    "        train_and_test(df3, user['id']) # fitted to train dataset\n",
    "        \n",
    "    # now using all the data to train random forest\n",
    "    rf = train_rf(df3, user['id'], print_importance=True)\n",
    "    \n",
    "    ## QUERYING JOKE PREDICTIONS FOR NEW USER\n",
    "    preds = get_preds(user, df3, rf, num_pred_jokes)\n",
    "    \n",
    "    return preds # preds contains all the joke predicted scores\n",
    "\n",
    "p = query(sample_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    query(sample_user)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
